---
title: "Retrieval Augmented Generation (RAG) in LangChain: Architecture & Practical Example"
slug: "rag-in-langchain"
date: 2026-01-26
description: "A complete guide to Retrieval Augmented Generation (RAG) in LangChain, explaining offline ingestion, online retrieval, vector stores, embeddings, and real-world implementation using a YouTube summarizer chatbot."
showToc: true
weight: 10
series: ["LangChain"]
categories: ["AI", "LLM", "Python"]
tags: ["LangChain", "RAG", "Vector Store", "Embeddings", "Chroma", "YouTube Summarizer"]
summary: "Learn how Retrieval Augmented Generation (RAG) works in LangChain with clear architecture diagrams and a complete YouTube summarizer chatbot implementation using Chroma, Ollama, and runnable chains."
---


# RAG
Rag is a way to make model smarter by giving it extra information at the time you ask your question.

## Simple Architecture Diagram
```mermaid
graph TD
    %% Node Definitions for better organization
    subgraph Data_Preparation ["Data Ingestion (Offline)"]
        direction LR
        Source["Raw Data Source"] --> Loader["Document Loader"]
        Loader --> Splitter["Text Splitter"]
        Splitter --> Embed_I["Embedding Model"]
        Embed_I --> VS[("Vector Store")]
    end

    subgraph Inference_Flow ["Retrieval & Generation (Online)"]
        Query["User Query"] --> Embed_Q["Embedding Model"]
        Embed_Q --> Retriever["Retriever"]
        VS -.->|"Vector Search"| Retriever
        Retriever --> Context["Relevant Chunks"]
        
        Context --> LLM["LLM Generator"]
        Query ---|Original Query| LLM
        LLM --> Answer["Final Answer"]
    end

    %% Visual separators
    style VS stroke-width:4px
```

## Detailed Architecture 
```mermaid
graph TD
    %% OFFLINE INGESTION PHASE
    subgraph Ingestion_Pipeline ["1. Data Ingestion (Offline Preparation)"]
        direction TB
        Source[/"Raw Data Sources<br/>(PDF, Docs, Web)"/] --> Loader["Document Loader<br/>(Extracts Text)"]
        Loader --> Splitter{"Text Splitter"}
        
        subgraph Chunking_Process ["Granular Processing"]
            direction LR
            C1["Chunk 1"] --> E1["Embedding Model"]
            C2["Chunk 2"] --> E2["Embedding Model"]
            C3["Chunk N"] --> E3["Embedding Model"]
        end
        
        Splitter --> C1 & C2 & C3
        
        E1 --> V1["Vector 1"]
        E2 --> V2["Vector 2"]
        E3 --> V3["Vector N"]
    end

    V1 & V2 & V3 --> VS[("Vector Store DB<br/>(Indexing)")]

    %% ONLINE RETRIEVAL PHASE
    subgraph Retrieval_Pipeline ["2. Search & Retrieval (Online Runtime)"]
        direction TB
        UserQ["User Query Text"] --> Q_Embed["Embedding Model<br/>(Same as Ingestion)"]
        Q_Embed --> Q_Vec["Query Vector"]
        
        subgraph Similarity_Search ["Vector Similarity Calculation"]
            direction LR
            Q_Vec --> Math{"Similarity Math<br/>(Cosine Similarity)"}
            VS --> Math
        end
        
        Math --> TopK["Top-K Chunks<br/>(Most Relevant)"]
    end

    %% GENERATION PHASE
    subgraph Generation_Phase ["3. Answer Generation"]
        TopK --> Prompt["Augmented Prompt<br/>(Context + Question)"]
        UserQ -.-> Prompt
        Prompt --> LLM["LLM Generator"]
        LLM --> Final["Final Answer"]
    end

    %% Legend/Style
    style VS stroke-width:4px
    style Math stroke-dasharray: 5 5
```

## Structure for making youtube summarizer chatbot

```mermaid
graph TD
    %% DATA EXTRACTION
    subgraph Video_Processing ["1. Video Data Extraction"]
        direction TB
        URL[/"YouTube URL: Ilg3gGewQ5U"/] --> YT_Loader["YoutubeLoader (LangChain)"]
        YT_Loader --> Raw_Docs["Document Object (Transcript)"]
    end

    %% PROCESSING & INGESTION
    subgraph Ingestion_Pipeline ["2. Processing & Embedding"]
        direction TB
        Raw_Docs --> Splitter["RecursiveCharacterTextSplitter<br/>(Chunk: 1000, Overlap: 0)"]
        
        subgraph Chunking_Logic ["Text Chunking & Vectorization"]
            direction LR
            Chunks["Text Chunks"] --> Emb_Model["OllamaEmbeddings<br/>(embeddinggemma)"]
        end
        
        Splitter --> Chunks
        Emb_Model --> VS[("Chroma DB <br/>(./chroma_db)")]
    end

    %% RETRIEVAL PHASE
    subgraph Retrieval_Pipeline ["3. Retrieval (Runnable)"]
        direction TB
        UserQ["User Question:<br/>'What is stochastic gradient descent?'"] --> Retriever["Vector Store Retriever<br/>(Similarity Search, K=5)"]
        VS --> Retriever
        Retriever --> Context["Related Transcript Docs (joined)"]
    end

    %% GENERATION PHASE
    subgraph Generation_Phase ["4. LLM Generation"]
        Context --> Prompt["Prompt Template<br/>(Expert Assistant Summary)"]
        UserQ --> Prompt
        Prompt --> LLM["ChatOllama<br/>(llama3, Temp 0.7)"]
        LLM --> Parser["StrOutputParser"]
        Parser --> Final["Final Answer Content"]
    end

    %% Styling
    style VS fill:#4ea8de,stroke:#333,stroke-width:2px,color:#fff
    style YT_Loader fill:#ff0000,color:#fff
    style LLM fill:#6c757d,color:#fff
    style Final fill:#2ecc71,color:#fff
    style UserQ fill:#f39c12,color:#fff
```



```python
from langchain_community.document_loaders import YoutubeLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama.embeddings import OllamaEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_chroma import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama.chat_models import ChatOllama as Ollama
from random import randint

# Initialize the embedding model
embedding_model= OllamaEmbeddings(model="embeddinggemma")

# Initialize the YouTube loader with the desired URL
loader = YoutubeLoader.from_youtube_url("https://www.youtube.com/watch?v=Ilg3gGewQ5U")

# Initialize the text splitter
splitted_text = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=0
)

# Initialize the vector store
vector_store = Chroma(
    collection_name=f"youtube_embeddings_{randint(0,10000)}",
    embedding_function=embedding_model,
    persist_directory="./chroma_db"
)


#initialize the LLM
llm= Ollama(model="llama3", temperature=0.7)



# Initialize Prompt Template
prompt_template= PromptTemplate(
    template="""
You are an expert assistant that helps to summarize YouTube videos.

user Question:
{question}

Given the following extracted parts of a YouTube video transcript, 
Transcript:
{transcript}

Please answer the question asked by user based on the transcript above.
""",
input_variables=["question", "transcript"]
)


# Initialize Structured Output Parser
output_parser= StrOutputParser()

# create a runnable retriever chain
vector_store_runnable= vector_store.as_retriever(
    kwargs={
        "search_kwargs": {"k": 5}
    },
    search_type="similarity"
)

question = "WHat is stochastic gradient descants."

# Load Documents from YouTube
documents = loader.load()

# Split Documents into Chunks
chunk = splitted_text.split_documents(documents)

# Add Chunks to Vector Store
vector_store.add_documents(chunk)

# Load Related Documents
related_docs = "".join([content.page_content for content in vector_store_runnable.invoke(question)])


# Invoke the prompt with the related documents
prompt = prompt_template.invoke(
    input={
        "question": question,
        "transcript": related_docs
    }
)

# Send to llm
response = llm.invoke(prompt)

print("Summary of the YouTube Video:")
print(response.content)
```

using chains:
```python
from langchain_community.document_loaders import YoutubeLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama.embeddings import OllamaEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_chroma import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama.chat_models import ChatOllama as Ollama
from langchain_core.runnables import RunnableParallel, RunnableSequence, RunnableLambda,RunnablePassthrough
from random import randint

# Initialize the embedding model
embedding_model= OllamaEmbeddings(model="embeddinggemma")

# Initialize the YouTube loader with the desired URL
loader = YoutubeLoader.from_youtube_url("https://www.youtube.com/watch?v=Ilg3gGewQ5U")

# Initialize the text splitter
splitted_text = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=0
)

# Initialize the vector store
vector_store = Chroma(
    collection_name=f"youtube_embeddings_{randint(0,10000)}",
    embedding_function=embedding_model,
    persist_directory="./chroma_db"
)


#initialize the LLM
llm= Ollama(model="llama3", temperature=0.7)



# Initialize Prompt Template
prompt_template= PromptTemplate(
    template="""
You are an expert assistant that helps to summarize YouTube videos.

user Question:
{question}

Given the following extracted parts of a YouTube video transcript, 
Transcript:
{transcript}

Please answer the question asked by user based on the transcript above.
""",
input_variables=["question", "transcript"]
)


# Initialize Structured Output Parser
output_parser= StrOutputParser()

# create a runnable retriever chain
vector_store_runnable= vector_store.as_retriever(
    kwargs={
        "search_kwargs": {"k": 5}
    },
    search_type="similarity"
)

question = "WHat is stochastic gradient descants."

# Load Documents from YouTube
documents = loader.load()

# Split Documents into Chunks
chunk = splitted_text.split_documents(documents)

# Add Chunks to Vector Store
vector_store.add_documents(chunk)

# Load Related Documents
related_docs = "".join([content.page_content for content in vector_store_runnable.invoke(question)])


# Invoke the prompt with the related documents
prompt = prompt_template.invoke(
    input={
        "question": question,
        "transcript": related_docs
    }
)

parallel_runnable = RunnableParallel({
    "question": RunnablePassthrough(),
    "transcript": vector_store_runnable | RunnableLambda(lambda x: "".join([content.page_content for content in x]))
}
)

# Create a sequence runnable to chain the prompt and llm
sequence_runnable = parallel_runnable | prompt_template | llm | output_parser

# Invoke the sequence runnable with the question
response = sequence_runnable.invoke(question)

print("Response:", response)
```