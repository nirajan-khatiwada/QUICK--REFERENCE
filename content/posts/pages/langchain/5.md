---
title: "Runnables in LangChain: Building Modular & Composable LLM Workflows"
slug: "runnables-in-langchain"
date: 2025-12-29
description: "Detailed guide to LangChain Runnables, covering RunnableSequence, RunnableParallel, RunnableBranch, RunnableLambda, RunnablePassthrough, and LCEL for building composable LLM pipelines."
showToc: true
weight: 5
series: ["Langchain"]
categories: ["AI", "LLM", "Python"]
tags: ["LangChain", "Runnables", "LCEL", "RunnableSequence", "RunnableParallel", "RunnableBranch", "Python"]
summary: "Learn how LangChain Runnables work and how to compose flexible, reusable, and scalable LLM workflows using sequences, parallel execution, branching logic, and LCEL syntax."
---

# Runnables
Runnable in LangChain is an abstraction that represents any component or operation that can be executed as part of a chain or workflow. Runnables can be simple functions, prompt templates, language models, or even complex chains themselves. They provide a unified interface for executing different types of operations within LangChain.

The runnable have common interface which means one runnable can be connected to another runnable as long as the output of the first runnable matches the input of the second runnable because of which we can create a chain

The output of runnable is also a runnable which means we can chain multiple runnables together to create complex workflows.

Also it have standard methods like
- invoke: This method is used to execute the runnable with the given input and return the output.
- batch_invoke: This method is used to execute the runnable with a batch of inputs and return a list of outputs.
- stream: This method is used to execute the runnable in a streaming fashion, yielding outputs as they are generated.

Runnables come in two flavors:
- Task Specific Runnables:
This are the core langchain component that has been converted into runnable so that can be used in chain/pipeline.
Examples:
- ChatOllama
- PromptTemplate
- OutputParser
- Retrievers

- Runnable Premitives:
They are use to connect typespecific runnables together to create complex workflows.
Examples:
- RunnableSequence : Used to chain multiple runnables in sequence.(| operator)
- RunnableParallel : Used to run multiple runnables in parallel.
- RunnableBranch : Used to create conditional branches based on input or intermediate results.
- RunnableLambda : Used to wrap simple functions as runnables.
- RunnablePassthrough : Used to pass input directly to output without any processing.
- RunnableMap : Used to apply a runnable to each item in a list of inputs.


1. RunnableSequence
IT is used to chain multiple runnables in sequence, where the output of one runnable becomes the input to the next runnable. It allows you to create linear workflows by connecting different components together.

```python
from langchain_ollama.chat_models import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableSequence
llm = ChatOllama(model="llama3")

template = PromptTemplate(template="What is the capital city of {country}. Tell in one word",input_variables=['country'])
parser = StrOutputParser()

output = RunnableSequence(template,llm,parser)

print(output.invoke({
    'country':"nepal"
}))
```
> Note: Here we have 3 runnables PromptTemplate, ChatOllama and StrOutputParser which are chained together using RunnableSequence which is also a runnable.

It can be done using | operator as well
```python
output = template | llm | parser
```
> Note: Previously we read this as Chain actually Chain is built on top of RunnableSequence with additional features.And each component in chain is a runnable note that the output is also a runnable.


2. RunnableParallel
RunnableParallel is used to run multiple runnables simultaneously, where each runnable processes the same input independently. It allows you to create parallel workflows by executing different components in parallel and return their outputs as a dictionary.

Example:
```python
from langchain_ollama.chat_models import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableSequence,RunnableParallel
llm = ChatOllama(model="llama3")

template1 = PromptTemplate(template="Generate a linked post of {topic}. Tell in one Sentence ",input_variables=['topic'])
template2 = PromptTemplate(template="Generate a linked post of {topic}. Tell in one Sentence ",input_variables=['topic'])
parser = StrOutputParser()

output = RunnableParallel({
    'tweet':RunnableSequence(
        template1,
        llm,
        parser),
    'facebook_post':RunnableSequence(
        template2,
        llm,
        parser)
})

result = output.invoke({'topic':'AI in Healthcare'})
print(result)
```

Note: Here we have two runnables tweet and facebook_post which are run in parallel using RunnableParallel which is also a runnable

**Extra** : We can merge the output of RunnableParallel using another runnable as shown below:
```python
from langchain_ollama.chat_models import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableSequence,RunnableParallel
llm = ChatOllama(model="llama3")

template1 = PromptTemplate(template="Generate a linked post of {topic}. Tell in one Sentence ",input_variables=['topic'])
template2 = PromptTemplate(template="Generate a linked post of {topic}. Tell in one Sentence ",input_variables=['topic'])
merge_template = PromptTemplate(
    template="Merge the following social media posts into a single post:\nTweet: {tweet}\nFacebook Post: {facebook_post}\nFinal Post:",
    input_variables=['tweet','facebook_post']
)

parser = StrOutputParser()

output = RunnableParallel({
    'tweet':RunnableSequence(
        template1,
        llm,
        parser),
    'facebook_post':RunnableSequence(
        template2,
        llm,
        parser)
})

merge_runnable = RunnableSequence(
    output,
    merge_template,
    llm,
    parser
)
final_result = merge_runnable.invoke({'topic':'AI in Healthcare'})
print(final_result)
```


3.RunnablePassthrough
RunnablePassthrough is used to pass the input directly to the output without any processing. It acts as a simple pass-through component that can be useful in certain scenarios where you want to maintain the input data unchanged.It can be used in conditional chains or as a default case in branching scenarios.

Example:
```python
output = RunnablePassthrough()
res=output.invoke("Hello, World!") 
print(res)  # Output: Hello, World!
```

Example:
```python
from langchain_ollama.chat_models import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableSequence,RunnableParallel,RunnablePassthrough
llm = ChatOllama(model="llama3")

template1 = PromptTemplate(template="Generate a linked topic for {topic}. Tell in one word ",input_variables=['topic'])
template2 = PromptTemplate(template="Explain the  {title}. Tell in one Sentence ",input_variables=['title'])
parser = StrOutputParser()

series1 = RunnableSequence(template1,llm,parser)
series2 = RunnableParallel({
    'title':RunnablePassthrough(),
    'explanation':RunnableSequence(template2,llm,parser)
})

final_chain = RunnableSequence(series1,series2)


result = final_chain.invoke({'topic':'''usage_metadata={'input_tokens': 24, 'output_tokens': 130, 'total_tokens': 154}), 'facebook_post': AIMessage(content='Here is a linked post on AI in Healthcare:\n\n**"Revolutionizing Patient Care: How Artificial Intelligence is Transforming Healthcare, from Personalized Medicine to Accurate Diagnoses and Improved Treatment Outcomes."**\n\nThis linked post explores the various ways AI is transforming healthcare, including:\n\n1. **Personalized Medicine**: AI-powered genomics and precision medicine enable tailored treatment plans for patients.\n2. **Accurate Diagnoses**: AI-assisted diagnostic tools reduce errors and improve patient outcomes by analyzing medical images and data.\n3. **Improved Treatment Outcomes**: AI-driven insights inform clinical decisions, optimizing treatment plans and reducing healthcare costs.\n\nWould you like me to expand on any of these topics or provide more information?', additional_kwargs={}, response_metadata={'model': 'llama3', 'created_at': '2026-01-20T06:36:42.8074196Z', 'done': True, 'done_reason': 'stop', 'total_duration': 43638602200, 'load_duration': 455596000, 'prompt_eval_count': 24, 'prompt_eval_duration': 130006700, 'eval_count': 145, 'eval_duration': 22614734100, 'logprobs': None, 'model_name': 'llama3', 'model_provider': 'ollama'}, id='lc_run--019bda1e-053c-7ee2-968d-e4a50b8579b2-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 24, 'output_tokens': 145, 'total_tokens': 169})}'''})
print(result)
```


4.RunnableLambda
It allows you create a custom python function and wrap it as a runnable so that it can be used in chain/pipeline.

It acts as middleware to perform custom operations on the input data before passing it to the next runnable in the chain.

Example:
```python
from langchain_ollama.chat_models import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableSequence,RunnableParallel,RunnablePassthrough,RunnableLambda
llm = ChatOllama(model="llama3")

prompt = PromptTemplate.from_template("What is a good name for a company that makes {product}?")

parser = StrOutputParser()


chain = RunnableSequence(
    prompt,
    llm,
   parser
)

final_output = RunnableParallel(
    {
        "output":RunnablePassthrough(),
        "length":RunnableLambda(lambda x: len(x))
    }
)

serial_output = RunnableSequence(
    chain,
    final_output
)

result = serial_output.invoke({"product":"colorful socks"})
print(result)  
```
> Note: It can be used to take user input and send userinput and model response to next runnable in chain.



5.RunnableBranch
RunnableBranch is used to create conditional branches in a workflow, allowing you to direct the flow of execution based on certain conditions or criteria. It enables dynamic decision-making within a chain by evaluating conditions and selecting the appropriate branch to execute.

Example:
```python
from langchain_ollama.chat_models import ChatOllama
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableBranch, RunnableLambda, RunnableSequence
from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser
from pydantic import BaseModel, Field
from typing import Literal


class PositiveNegative(BaseModel):
    feedback: Literal["positive", "negative", "none"] = Field(
        description="Analyze if feedback is positive or not", default="none"
    )


llm = ChatOllama(model="llama3")
parser = PydanticOutputParser(pydantic_object=PositiveNegative)
strparser = StrOutputParser()
prompt1 = PromptTemplate(
    template="""Classify the following feedback as either "positive" or "negative". 
Respond ONLY with JSON in the format:
{format_instruction}
Feedback:
{feedback}
""",
    input_variables=["feedback"],
    partial_variables={"format_instruction": parser.get_format_instructions()},
)
prompt2 = PromptTemplate(
    template="Write a response for  for this positive fedback {feedback}.Write only feedback so i can send directly to customer ",
    input_variables=["feedback"],
)

prompt3 = PromptTemplate(
    template="Write a response for  for this negative fedback {feedback}.Write only feedback so i can send directly to customer",
    input_variables=["feedback"],
)

classifier_chain = RunnableSequence(prompt1, llm, parser)

branchchain = RunnableBranch(
    (lambda x: x.feedback == "positive", prompt2 | llm | strparser),  # positive branch
    (lambda x: x.feedback == "negative", prompt3 | llm | strparser),  # negative branch
    RunnableLambda(lambda x: "Coundnt find a sentimate"),  # default case
)

final_chain = classifier_chain | branchchain
response = final_chain.invoke({"feedback": "This is bad phone"})
print(response)

```


LCEL
LCEL allows use to define the RunnableSequence using a declarative syntax. It provides a more readable and concise way to define chains of runnables.

Example:
```python
output = RunnableSequence(prompt,llm,parser)
```

Can be written as
```python
output = prompt | llm | parser
```

THis is only supported for RunnableSequence currently.